In a recent informal experiment, Canon Lab Australia invited five professional photographers to spend a few minutes with the same man and “register” his essence. Each photographer was given false information about the person, and this false information led to dramatically different photographs. An essence characterizes a substance or a form, in the sense of the forms and ideas in Platonic idealism. It is permanent, unalterable, and eternal, and is present in every possible world. Classical humanism has an essentialist conception of the human, in its endorsement of the notion of an eternal and unchangeable human nature. This has been criticized by Kierkegaard, Marx, Heidegger, Sartre, and many other existential and materialist thinkers. In Plato's philosophy (in particular, the Timaeus and the Philebus), things were said to come into being by the action of a demiurge who works to form chaos into ordered entities. Many definitions of essence hark back to the ancient Greek hylomorphic understanding of the formation of the things. According to that account, the structure and real existence of any thing can be understood by analogy to an artefact produced by a craftsperson. The craftsperson requires hyle (timber or wood) and a model, plan or idea in her own mind, according to which the wood is worked to give it the indicated contour or form (morphe). Aristotle was the first to use the terms hyle and morphe. According to his explanation, all entities have two aspects: "matter" and "form". It is the particular form imposed that gives some matter its identity—its quiddity or "whatness" (i.e., its "what it is"). Plato was one of the first essentialists, postulating the concept of ideal forms—an abstract entity of which individual objects are mere facsimiles. To give an example: the ideal form of a circle is a perfect circle, something that is physically impossible to make manifest; yet the circles we draw and observe clearly have some idea in common—the ideal form. Plato proposed that these ideas are eternal and vastly superior to their manifestations, and that we understand these manifestations in the material world by comparing and relating them to their respective ideal form. Plato's forms are regarded as patriarchs to essentialist dogma simply because they are a case of what is intrinsic and a-contextual of objects—the abstract properties that makes them what they are. (For more on forms, read Plato's parable of the cave.) Karl Popper splits the ambiguous term realism into essentialism and realism. He uses essentialism whenever he means the opposite of nominalism, and realism only as opposed to idealism. Popper himself is a realist as opposed to an idealist, but a methodological nominalist as opposed to an essentialist. For example, statements like "a puppy is a young dog" should be read from right to left, as an answer to "What shall we call a young dog"; never from left to right as an answer to "What is a puppy?" Concerns have been raised about the potential use of GAN-based human image synthesis for sinister purposes, e.g., to produce fake and/or incriminating photographs and videos. GANs can be used to generate unique, realistic profile photos of people who do not exist, in order to automate creation of fake social media profiles. Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network. A new website calls attention to the fact that artificial intelligence can fake photos of human faces well enough to fool people. Once the network is trained, we can feed in a photo, and we get out the year in which the system guesses it was taken. For example, for the following two photos ChronoNet guesses 1951 (left) and 1971 (right). Convolutional neural networks are very general and very powerful. As an example, consider Ilya Kostrikov and Tobias Weyand’s ChronoNet, a CNN that guesses the year in which a photo was taken. Since public sources can provide large numbers of digitally archived photos taken over the past century with known dates, it’s relatively straightforward to obtain labeled data (dated photos, in this case) with which to train this network. Modern, sophisticated machine learning techniques like convolutional neural networks (CNNs) have many millions of parameters, hence need a great deal of training data to avoid overfitting. Obtaining enough labelled data to both train and test a system is often the greatest practical challenge facing a machine learning researcher. Every machine learning system has parameters — or there is nothing to learn. Simple systems may have only a handful. Increasing the number of parameters can allow a system to learn more complex relationships, making for a more powerful learner and, if the relationships between input and output are complex, a lower error rate. On the other hand, more parameters also allow a system to memorize more of the training data, hence overfit more easily. This means that there is a relationship between the number of parameters and the amount of training data needed. One technical pitfall to guard against is overfitting . This happens when the machine is able to memorize the right answers to individual training examples without generalizing, meaning learning an underlying pattern that will hold when tested on different data. The simplest way to avoid overfitting is simply to test the performance of the system on a random subset of the labelled data that is “held out”, meaning not used during training. If the system’s performance on this test data is roughly as good as on the training data, then one can feel confident that the system really has learned how to see a general pattern in the data, and hasn’t just memorized the training examples. This is the same as the rationale for giving students a midterm exam with questions they haven’t seen before, rather than just reusing examples that have been worked through in class. The relationship between the photo and the response is determined by a set of parameters, which are tuned during a learning phase — hence “machine learning”. The most common approach is supervised learning , which involves working through a large number of labelled examples — that is, example images paired with the desired output for each. When the parameters are set to random values, the machine will only get the answer right by pure chance; but even given a random starting point, one can slowly vary one or more parameters and ask, “is this variation better, or worse?” In this way, by playing a game of Marco Polo with parameters, a computer can optimize itself to learn the task. A typical training program involves trying millions, billions, or trillions of parameter choices, all the while steadily improving performance on the task. Eventually the improvement levels off, telling us that the accuracy has probably gotten as good as it’s going to get, given the inherent difficulty of the task and the limitations of the machine and the data. Computers can analyze the physical features of a person by making calculations based on their picture. This is an example of the more general problem of image understanding: a computer program analyzes a photo, makes a determination about the photo, then emits some kind of meaningful judgement (say, “the person in this photo is likely between the ages of 18 and 23”). In an era of pervasive cameras and big data, machine-learned physiognomy can also be applied at unprecedented scale. Given society’s increasing reliance on machine learning for the automation of routine cognitive tasks, it is urgent that developers, critics, and users of artificial intelligence understand both the limits of the technology and the history of physiognomy, a set of practices and beliefs now being dressed in modern clothes. Hence, we are writing both in depth and for a wide audience: not only for researchers, engineers, journalists, and policymakers, but for anyone concerned about making sure AI technologies are a force for good. Many of us in the research community found Wu and Zhang’s analysis deeply problematic, both ethically and scientifically. In one sense, it’s nothing new. However, the use of modern machine learning (which is both powerful and, to many, mysterious) can lend these old claims new credibility. A recent case in point is Xiaolin Wu and Xi Zhang’s paper , “Automated Inference on Criminality Using Face Images”, submitted to arXiv (a popular online repository for physics and machine learning researchers) in November 2016. Wu and Zhang’s claim is that machine learning techniques can predict the likelihood that a person is a convicted criminal with nearly 90% accuracy using nothing but a driver’s license-style face photo. Although the paper was not peer-reviewed, its provocative findings generated a range of press coverage. Rapid developments in artificial intelligence and machine learning have enabled scientific racism to enter a new era, in which machine-learned models embed biases present in the human behavior used for model development. Whether intentional or not, this “laundering” of human prejudice through computer algorithms can make those biases appear to be justified objectively. The practice of using people’s outer appearance to infer inner character is called physiognomy. While today it is understood to be pseudoscience , the folk belief that there are inferior “types” of people, identifiable by their facial features and body measurements, has at various times been codified into country-wide law, providing a basis to acquire land, block immigration, justify slavery, and permit genocide. When put into practice, the pseudoscience of physiognomy becomes the pseudoscience of scientific racism . On a scientific level, machine learning can give us an unprecedented window into nature and human behavior, allowing us to introspect and systematically analyze patterns that used to be in the domain of intuition or folk wisdom. Seen through this lens, Wu and Zhang’s result is consistent with and extends a body of research that reveals some uncomfortable truths about how we tend to judge people. On a practical level, machine learning technologies will increasingly become a part of all of our lives, and like many powerful tools they can and often will be used for good — including to make judgments based on data faster and fairer. Machine learning can also be misused, often unintentionally. Such misuse tends to arise from an overly narrow focus on the technical problem, hence: Lack of insight into sources of bias in the training data; Lack of a careful review of existing research in the area, especially outside the field of machine learning; Not considering the various causal relationships that can produce a measured correlation; Not thinking through how the machine learning system might actually be used, and what societal effects that might have in practice. Wu and Zhang’s paper illustrates all of the above traps. This is especially unfortunate given that the correlation they measure — assuming that it remains significant under more rigorous treatment — may actually be an important addition to the already significant body of research revealing pervasive bias in criminal judgment. Deep learning based on superficial features is decidedly not a tool that should be deployed to “accelerate” criminal justice; attempts to do so, like Faception’s, will instead perpetuate injustice. The federal reserve had plenty to fret about as it prepared to discuss policy interest rates on September 17th and 18th. Trade tensions and wilting global growth have seen businesses cut back investment in the second quarter of the year. In manufacturing, production and capacity utilisation have been falling since the end of 2018. Though the Fed has described jobs growth as “solid”, some analysts worry that the labour market is wobbling. As expected, these concerns prompted the central bank to lower rates for the second time this year, by 0.25 percentage points, to a target of 1.75-2%. But the meeting was overshadowed by turmoil in money markets.On September 17th, for the first time in a decade, the Fed injected cash into the short-term money market. The intervention was needed after the federal funds rate, at which banks can borrow from each other, climbed above the Fed’s target. It rose as the “repo” rate—the price at which high-quality securities such as American government bonds can be temporarily swapped for cash—hit an intra-day peak of over 10%.The Federal Reserve cut US interest rates by 25 basis points for the third time this year but signalled that it has finished easing monetary policy for the time being, pending clearer economic data.The US central bank on Wednesday said that uncertainty on the economic outlook justified its latest cut but chairman Jay Powell said that a preliminary US-China trade deal and lower risk of a no-deal Brexit had the potential to increase business confidence.The pause does not mean the Fed has any plans to resume the rate increases it pursued until the end of 2018, Mr Powell said at a press conference. “I think we would need to see a really significant move up in inflation that’s persistent before we would consider raising rates to address inflation,” he said. After a two-day meeting in Washington, the Fed’s rate-setting committee made two significant changes to the language of its monetary policy statement. It said it would “assess the appropriate path” for rates instead of saying it would “act as appropriate to sustain the expansion”. My guess is that we are still early in the incremental phase with ML — in the “let’s see how it works” stage. And how is it working? A good place to start is to talk to those quants obsessed by factor-based investing, where large data sets exist and there are many competing narrative methodologies all tested to destruction by academics. The promise is obvious. ML should be able to overcome the behavioural biases that still exist in factor-based investing despite the wall of data. Even here, though, you will find plenty of warning signs. Peruse the pages of industry publications such as the Journal of Financial Data Science and you will find the likes of Joseph Simonian, director of quantitative research at Natixis, who observed (to Risk.net) that “many people on the Street think that to do financial data science, you just take a machine-learning algorithm and a data science model, and wholesale apply it to finance. But our argument, and the basis of this journal, is that financial data . . . has its own peculiarities.” The challenge? Capital markets are constantly adaptive and data sets may not be deep and long enough to reveal any enduring trends. Commentators such as Rob Arnott of Research Affiliates and Campbell Harvey, a professor at the Fuqua School of Business in North Carolina, have already proposed a checklist for applying ML techniques with a focus on the depth of those data sources. In the absence of this, Mr Arnott suggested to Risk.net that using sparse data to train ML algorithms was akin to driving a Ferrari on a dirt track. “The machine has no idea in 2007 that a financial crisis is on the way when it is building its model in 2006,” says Mr Lapthorne. “Equally, the machine can feed back its interpretation of our data to help us understand our existing quant models. It is also excellent at recognising non-linear factors such as balance sheet risk.” And here is one last crucial insight. It seems that ML routines can be especially useful on working out what to buy (and sell) in regime changes, ie, big shifts in interest rates. Unanchored to the past and with no look-ahead bias they become especially powerful, although that needs to be balanced by the observation that during periods of pronounced volatility the models don’t seem to keep up as well. But over the past five years or so, Facebook has built a team of “hundreds” of machine learning experts, engineers and data scientists to develop algorithms that can automatically flag unwanted content. According to Mr Schroepfer, technologies for image recognition — which were unreliable before 2014 — are now “stunningly good”. Language understanding, which was introduced for hate speech in 2017 for example, is improving, but still fairly nascent as algorithms struggle to account for context. But the system needs to be trained. The more data that is fed into it — whether images of terrorist insignia or harmful keywords — the more the machine learning technology learns and improves. Without enough training data, the system does not know what to look for. A recent example was when Facebook said it did not have enough first-person shooter video footage for its algorithms to recognise and take down the videos of the attacks on two mosques in New Zealand earlier this year. Facebook has now equipped London police with body cameras during terrorist training exercises to get more footage, having eschewed using footage of video game shoot-outs or paintballing. 


